{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aagarwal17/datasci-207-arun/blob/main/Module_6_FFNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4-bBPLBkXDQ"
      },
      "source": [
        "## Week 6: Feedforward NN\n",
        "\n",
        "Instructor: Cornelia Paulik <br>\n",
        "Email: cilin@ischool.berkeley.edu <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyVUALBlkXDR"
      },
      "source": [
        "---\n",
        "#### ``Readings``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YphHy4XskXDS"
      },
      "source": [
        "1. Raschka & Mirjalili (RM), 3rd edition: Chapter 12 (pp. 383-423), Chapter 13 (pp.462-470) in the print version of the book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_D4DK5kkXDS"
      },
      "source": [
        "#### ``Learning objectives``\n",
        "\n",
        "1. Apply knowledge of deep learning by implementing a  FFNN to classify diabetic retinopathy from retinal fundus images captured under varying imaging conditions.\n",
        "2. Analyze the impact of image transformation and data augmentation techniques on model performance by evaluating their effectiveness in improving generalization and reducing overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLBvOjcPkXDS"
      },
      "source": [
        "#### ``Motivation``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHP0FJmbkXDT"
      },
      "source": [
        "Diabetic retinopathy (DR) is an eye condition that  affects blood vessels in the retina. It can cause vision loss and blindness in people who have diabetes. Screening for DR allows earlier and more effective treatment options for millions of people.\n",
        "\n",
        "A deep-learning approach to predicting DR from eye images was proposed in 2016, in the paper <span style=\"color:chocolate\">Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs</span> published in JAMA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VPEn2hblkXDT",
        "outputId": "f69cdaff-92ca-42e7-cf08-851f3eb9df00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image source: https://www.sciencedirect.com/science/article/pii/S0010482513002862#f0005 \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './images/DR.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d1062a02ca73>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Image source: https://www.sciencedirect.com/science/article/pii/S0010482513002862#f0005 \\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./images/DR.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1232\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './images/DR.png'"
          ]
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "print('Image source: https://www.sciencedirect.com/science/article/pii/S0010482513002862#f0005 \\n')\n",
        "Image(filename='./images/DR.png', width=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf_s2iOYkXDU"
      },
      "source": [
        "Typical fundus images: (a) Normal; (b) Mild DR; (c) Moderate DR; (d) Severe DR; (e) Prolific DR; (f) Macular edema.\n",
        "\n",
        "\n",
        "<span style=\"color:chocolate\">Microaneurysms:</span> are tiny outpouchings of blood that protrude from an artery or vein. When they occur in the eye, they are known as retinal microaneurysms. If these protrusions open, they leak blood into the tissues of the retina\n",
        "\n",
        "<span style=\"color:chocolate\">Exuades:</span> a mass of cells and fluid that has seeped out of blood vessels or an organ, especially in inflammation.\n",
        "\n",
        "<span style=\"color:chocolate\">Macula:</span> an oval yellowish area surrounding the fovea near the center of the retina in the eye, which is the region of keenest vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACVxOClWkXDU"
      },
      "source": [
        "#### ``Data``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zesjVsDtkXDU"
      },
      "source": [
        "We will use a small dataset of retina images (`Download` links: [images](https://drive.google.com/drive/folders/1sdfUC64Un1iwuiHEehcbijxB54OhU_nd?usp=sharing) and [labels](https://drive.google.com/drive/folders/1MOlSJBZg7L1HtG5vHPt77ighRvQaGfDg?usp=sharing)). You will **build** and **train** a simple **NN model** to predict whether or not to refer a patient for DR treatment using binarized severity of DR in patients: no referral if {No DR, mild} and referral if {moderate, severe, and proliferate DR}.\n",
        "\n",
        "\n",
        "<u>Note</u>: the original dataset is hosted by Kaggle [[Source]](https://www.kaggle.com/c/aptos2019-blindness-detection/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgbEfHavkXDV"
      },
      "source": [
        "Import the necessary libraries and make sure to replace IMAGE_PATH and LABEL_PATH with the path to the directories where you saved the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BezDXRa9kXDV"
      },
      "source": [
        "I am using the following package versions:\n",
        "* Pandas: 2.2.2\n",
        "* Numpy: 1.26.4\n",
        "* Matplotlib: 3.8.4\n",
        "* Seaborn: 0.13.2\n",
        "* Sklearn: 1.4.2\n",
        "* IPython: 8.25.0\n",
        "* Mlxtend: 0.23.3\n",
        "* Tensorflow: 2.17.0\n",
        "* Statsmodels: 0.14.2\n",
        "* Shap: 0.46.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA536KQvkXDV"
      },
      "outputs": [],
      "source": [
        "# standard\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "# tf and keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# shap\n",
        "import shap\n",
        "\n",
        "# plots and images\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "\n",
        "#silence TF\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "\n",
        "random.seed(2)\n",
        "%matplotlib inline\n",
        "\n",
        "# FILL IN CODE HERE #\n",
        "IMAGE_PATH = './data/DiabeticRetinopathy/images/' # replace with your path\n",
        "LABEL_PATH = './data/DiabeticRetinopathy/labels/' # replace with your path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9W0ROHykXDV"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cfPLjHekXDV"
      },
      "source": [
        "Let's now explore our dataset. We will start with label inspection and continue with image visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQCWt377kXDV"
      },
      "outputs": [],
      "source": [
        "y = pd.read_csv(\n",
        "    LABEL_PATH + 'labels.csv'\n",
        ")\n",
        "\n",
        "print('Shape of labels:', y.shape)\n",
        "print('Unique diagnosis codes:', np.sort(y.diagnosis.unique()))\n",
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpd7behKkXDW"
      },
      "source": [
        "There are 200 training images corrresponding to 5 different diabetic retinopathy (DR) diagnosis codes:\n",
        "\n",
        "* No DR (0)\n",
        "* mild (1)\n",
        "* moderate (2)\n",
        "* severe (3)\n",
        "* proliferate DR (4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2hFEVm-kXDW"
      },
      "source": [
        "Image inspection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2Cu1_GskXDW"
      },
      "outputs": [],
      "source": [
        "# read image\n",
        "indx=0\n",
        "images = []\n",
        "\n",
        "print('Sample of images in data:')\n",
        "for idx, img in enumerate(os.listdir(IMAGE_PATH)):\n",
        "    img = load_img(\n",
        "    IMAGE_PATH + img)\n",
        "    images.append(img)\n",
        "\n",
        "nrows, ncols = 2,4 #print first 8 images\n",
        "f, axs = plt.subplots(nrows, ncols, figsize=(20,10))\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        indx = i*nrows+j\n",
        "        axs[i,j].imshow(images[indx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PZEsGgOkXDW"
      },
      "source": [
        "Like any real-world data set, you can see that these images have different sizes and focus. Later, you will use image transformation and data augmentation techniques to remove some of this variation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSYiD1OEkXDW"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybywG_LykXDX"
      },
      "source": [
        "The quality of the data determines how well a machine learning algorithm can learn. This section will apply some simple techniques to deal with class imbalance. We will then create training/validation/test datasets and perform image transformation and augmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwIsgjZ0kXDX"
      },
      "outputs": [],
      "source": [
        "# histogram\n",
        "sns.histplot(\n",
        "    data=y,\n",
        "    stat=\"count\",\n",
        "    x = \"diagnosis\"\n",
        ");\n",
        "\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7IORXSgkXDX"
      },
      "outputs": [],
      "source": [
        "# compute percentage of the largest class\n",
        "temp = y.groupby(\n",
        "    'diagnosis',\n",
        "    as_index=False\n",
        ").id_code.count()\n",
        "\n",
        "temp.rename(\n",
        "    columns={'id_code':'counts'},\n",
        "    inplace=True\n",
        ")\n",
        "\n",
        "temp['pct'] = temp.counts/temp.counts.sum()\n",
        "print('Percentage of the largest class:',\n",
        "      int(temp.pct.nlargest(n=1)[0]*100), '%'\n",
        "     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcNwk42XkXDX"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5pGl2qbkXDX"
      },
      "source": [
        "`Correct for data imbalance`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOzBuofskXDX"
      },
      "source": [
        "As you can see from the histogram above, our dataset is very imbalanced, which is common in healthcare, and it happens because some diseases are rare. The presence of imbalanced data hampers the detection of rare events as most classification methods implicitly assume a similar occurrence of classes and are designed to maximize the overall classification accuracy.\n",
        "\n",
        "We will correct for class imbalance in two ways:\n",
        "\n",
        "  * First, we will binarize the DR diagnosis as follows:\n",
        "     - 'no refer' are {No DR, mild}\n",
        "     - 'refer' are {Moderate, Severe, Proliferate}\n",
        "\n",
        "\n",
        "  * Second, we'll only take 80 random samples from the 'no refer' class and 80 from the 'refer' class.\n",
        "\n",
        "It is a crude method to deal with imbalanced data, but it will be good enough for starters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XprLeRSekXDX"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1234)\n",
        "\n",
        "no_refer = y[y.diagnosis.isin((0,1))]\n",
        "refer = y[y.diagnosis.isin((2,3,4))]\n",
        "\n",
        "# randomly draw 80 images from each classes\n",
        "temp_no_refer = list(np.random.choice(\n",
        "    no_refer.id_code,\n",
        "    size=80,\n",
        "    replace=False\n",
        "))\n",
        "\n",
        "temp_refer = list(np.random.choice(\n",
        "    refer.id_code,\n",
        "    size=80,\n",
        "    replace=False\n",
        "))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1GCgiz1kXDX"
      },
      "source": [
        "Next, we will use the **preprocess_data_part1()** function defined below to generate lists of images and labels (`images_mini` and `y_mini`) based on the values in the temp_no_refer and temp_refer lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYeIURWFkXDY"
      },
      "outputs": [],
      "source": [
        "def preprocess_data_part1(IMAGE_PATH, LABEL_PATH):\n",
        "    \"\"\" Generate lists of images and labelsbased on temp_no_refer and temp_refer lists\n",
        "\n",
        "    Params:\n",
        "    -------\n",
        "    IMAGE_PATH (str): path to directory with images.\n",
        "    LABEL_PATH (str): path to directory with labels.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    images_mini  (np.ndarray): Images of shape (N, 224, 224, 3)\n",
        "    y_mini (np.ndarray): Labels of shape (N,)\n",
        "    \"\"\"\n",
        "    y_mini = []\n",
        "    images_mini = []\n",
        "\n",
        "    # create lists of images and labels `images_mini` and `y_mini`\n",
        "    # based on temp_no_refer and temp_refer selections\n",
        "    for idx, img in enumerate(os.listdir(IMAGE_PATH)):\n",
        "        # read labels\n",
        "        if img.split('.')[0] in temp_no_refer:\n",
        "                y_mini.append(0)\n",
        "        elif img.split('.')[0] in temp_refer:\n",
        "                y_mini.append(1)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "\n",
        "        # read image\n",
        "        img = load_img(\n",
        "            IMAGE_PATH + img,\n",
        "            target_size=(224, 224)\n",
        "        )\n",
        "\n",
        "        # transform image to array\n",
        "        img = img_to_array(img)\n",
        "\n",
        "        # append to images\n",
        "        images_mini.append(img)\n",
        "\n",
        "    # stack images and trasnform to array\n",
        "    images_mini = np.stack(images_mini)\n",
        "    y_mini = np.array(y_mini).flatten()\n",
        "\n",
        "    return images_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyLLgHZskXDY"
      },
      "outputs": [],
      "source": [
        "# generate images and labels based on preprocess_data_part1() function\n",
        "images_mini, y_mini = preprocess_data_part1(IMAGE_PATH, LABEL_PATH)\n",
        "\n",
        "print(f\"images_mini shape {images_mini.shape}\")\n",
        "print(f\"y_mini shape {y_mini.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAOcOaf0kXDY"
      },
      "source": [
        "`Create train/validation/test data` and ``perform image tranformation and augmentation``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2VEt043kXDY"
      },
      "source": [
        "The next step in the data preprocessing part is to split the data into training, validation, and test sets. Once we have these partitions, we will apply image transformation and augmentations.\n",
        "\n",
        "\n",
        "To give you an idea of what image transformation and augmentation do, let's see an example applied to the first image in our mini data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgCWluxBkXDZ"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(1234)\n",
        "\n",
        "fig = plt.figure(figsize=(14, 12))\n",
        "\n",
        "# pull first image from data\n",
        "image = images_mini[0]\n",
        "\n",
        "# plot original\n",
        "ax = fig.add_subplot(1, 5, 1)\n",
        "ax.imshow(array_to_img(image))\n",
        "ax.set_title('Original', size=15);\n",
        "\n",
        "# resize\n",
        "ax = fig.add_subplot(1, 5, 2)\n",
        "img_resize = tf.image.resize(image, size=(224, 224))\n",
        "ax.imshow(array_to_img(img_resize))\n",
        "ax.set_title('Step 1: Resize', size=15);\n",
        "\n",
        "\n",
        "# adjust brightness\n",
        "ax = fig.add_subplot(1, 5, 3)\n",
        "img_bright = tf.image.adjust_brightness(img_resize, 0.3)\n",
        "ax.imshow(array_to_img(img_bright))\n",
        "ax.set_title('Step 2: Brightness', size=15);\n",
        "\n",
        "\n",
        "# adjust contrast\n",
        "ax = fig.add_subplot(1, 5, 4)\n",
        "img_contrast = tf.image.adjust_contrast(img_bright, contrast_factor=3)\n",
        "ax.imshow(array_to_img(img_contrast))\n",
        "ax.set_title('Step 3: Contrast', size=15);\n",
        "\n",
        "\n",
        "# flip left right\n",
        "ax = fig.add_subplot(1, 5, 5)\n",
        "img_flip = tf.image.flip_left_right(img_contrast)\n",
        "ax.imshow(array_to_img(img_flip))\n",
        "ax.set_title('Step 4: Flip left right', size=15);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXDRraLikXDZ"
      },
      "source": [
        "Next, we will define and run the **preprocess_data_part2()** function to create:\n",
        "\n",
        "* train/validation/test sets with split (0.6,0.2,0.2)\n",
        "\n",
        "* image transformation and augmentation, as follows:\n",
        "\n",
        "<u>Applied on training, validation and test sets</u>:\n",
        "  - resize to IMAGE_SIZE =(224,224) using tf.image.resize()\n",
        "  - normalize all pixel values to the range (0,1)\n",
        "  \n",
        "<u>Applied on training set only</u> (note that this step will create additional/augmented copies of the training data):\n",
        "  - adjust brightness by adding DELTA=0.3 to the pixel values using tf.image.adjust_brighness()\n",
        "  - adjust contrast to CONTRAST_FACTOR=3 using tf.image.adjust_contrast()\n",
        "  - flip left right using tf.image.flip_left_right()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1P40WOtkXDZ"
      },
      "source": [
        "The quantity and diversity of data gathered have a significant impact on the results of a neural net model. One can apply augmentations to artificially inflate the training dataset by warping the original data such that their label does not change. These augmentations can significantly improve learning results without collecting new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTErbcwKkXDZ"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = (224, 224)\n",
        "CONTRAST_FACTOR = 3\n",
        "DELTA = 0.3\n",
        "\n",
        "def preprocess_data_part2(images, y, split=(0.6,0.2,0.2)):\n",
        "    \"\"\" Split data into train, validation and test sets; apply transformaions and augmentations\n",
        "\n",
        "    Params:\n",
        "    -------\n",
        "    images  (np.ndarray): Images of shape (N, 224, 224, 3)\n",
        "    y (np.ndarray): Labels of shape (N,)\n",
        "    split (tuple): 3 values summing to 1 defining split of train, validation and test sets\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_train (np.ndarray): Train images of shape (N_train, 224, 224, 3)\n",
        "    y_train (np.ndarray): Train labels of shape (N_train,)\n",
        "    X_val (np.ndarray): Val images of shape (N_val, 224, 224, 3)\n",
        "    y_val (np.ndarray): Val labels of shape (N_val,)\n",
        "    X_test (np.ndarray): Test images of shape (N_test, 224, 224, 3)\n",
        "    y_test (np.ndarray): Test labels of shape (N_test,)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### create train/validation/test sets ###\n",
        "    #########################################\n",
        "    # NOTE: Each time you run this cell, you'll re-shuffle the data. The ordering will be the same due to the random seed generator\n",
        "    tf.random.set_seed(1234)\n",
        "    np.random.seed(1234)\n",
        "    shuffle = np.random.permutation(np.arange(images.shape[0]))\n",
        "    images, y = images[shuffle], y[shuffle]\n",
        "\n",
        "    splits = np.multiply(len(images_mini), split).astype(int)\n",
        "    X_train, X_val, X_test = np.split(images_mini, [splits[0], splits[0]+splits[1]])\n",
        "    y_train, y_val, y_test = np.split(y_mini, [splits[0], splits[0]+splits[1]])\n",
        "\n",
        "    ### image transformation on training, validation, and test data ###\n",
        "    ###################################################################\n",
        "    # image resize\n",
        "    X_train = tf.image.resize(X_train, size=IMAGE_SIZE)\n",
        "    X_val = tf.image.resize(X_val, size=IMAGE_SIZE)\n",
        "    X_test = tf.image.resize(X_test, size=IMAGE_SIZE)\n",
        "\n",
        "    # rescale image to [0,1], i.e., greyscale\n",
        "    X_train = tf.image.rgb_to_grayscale(X_train)/255.0\n",
        "    X_val = tf.image.rgb_to_grayscale(X_val)/255.0\n",
        "    X_test = tf.image.rgb_to_grayscale(X_test)/255.0\n",
        "\n",
        "\n",
        "    ### image augmentation on training data ###\n",
        "    ###########################################\n",
        "    # adjust brightness\n",
        "    X_train_augm = tf.image.adjust_brightness(X_train, delta=DELTA)\n",
        "\n",
        "    # adjust contrast\n",
        "    X_train_augm = tf.image.adjust_contrast(X_train_augm, contrast_factor=CONTRAST_FACTOR)\n",
        "\n",
        "    # random flip\n",
        "    X_train_augm = tf.image.random_flip_left_right(X_train_augm)\n",
        "\n",
        "    # concatenate original X_train and augmented X_train data\n",
        "    X_train = tf.concat([X_train, X_train_augm],axis=0)\n",
        "\n",
        "    # concatenate y_train (note the label is preserved)\n",
        "    y_train_augm = y_train\n",
        "    y_train = tf.concat([y_train, y_train_augm],axis=0)\n",
        "\n",
        "    # shuffle X_train and y_train, i.e., shuffle two tensors in the same order\n",
        "    shuffle = tf.random.shuffle(tf.range(tf.shape(X_train)[0], dtype=tf.int32))\n",
        "    X_train = tf.gather(X_train, shuffle)\n",
        "    y_train = tf.gather(y_train, shuffle).numpy() #also transforms y_train to numpy array\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4C84odJkXDa"
      },
      "source": [
        "Let's sanity check our implementation of the preprocess_data_part2() function by printing the shape of (X,y) from train, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEo8UzIkkXDa"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val, X_test, y_test = preprocess_data_part2(\n",
        "    images_mini,\n",
        "    y_mini,\n",
        "    split=(0.6,0.2,0.2)\n",
        ")\n",
        "\n",
        "print(f\"X_train shape {X_train.shape}\")\n",
        "print(f\"y_train shape {y_train.shape}\")\n",
        "print(f\"X_val shape {X_val.shape}\")\n",
        "print(f\"y_val shape {y_val.shape}\")\n",
        "print(f\"X_test shape {X_test.shape}\")\n",
        "print(f\"y_test shape {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toYY95qTkXDa"
      },
      "outputs": [],
      "source": [
        "X_train[0][1][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErQl0ph9kXDa"
      },
      "source": [
        "Let's also print out the first 8 train and validation examples with the label of each example as the title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40ru2CSEkXDa"
      },
      "outputs": [],
      "source": [
        "# print taining data\n",
        "print('Print training data examples:')\n",
        "nrows, ncols = 1,4 #print first 4 images\n",
        "f, axs = plt.subplots(nrows, ncols, figsize=(14,12))\n",
        "for i in range(ncols):\n",
        "    axs[i].imshow(array_to_img(X_train[i]))\n",
        "    axs[i].set(title=y_train[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAXLLk_UkXDb"
      },
      "outputs": [],
      "source": [
        "# print test data\n",
        "print('Print validation data examples:')\n",
        "nrows, ncols = 1,4 #print first 4 images\n",
        "f, axs = plt.subplots(nrows, ncols, figsize=(14,12))\n",
        "for i in range(ncols):\n",
        "    axs[i].imshow(array_to_img(X_val[i]))\n",
        "    axs[i].set(title=y_val[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjzLyFJFkXDb"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ScDO0MxkXDb"
      },
      "source": [
        "Our objective is to build and train a FFNN model to refer patients to doctors based on the severity of DR seen in these images. We are interested in exploring binary classification of 'no refer' and 'refer'.\n",
        "\n",
        "<u>The architecture of our NN model is as follows</u>:\n",
        "\n",
        "1. the model receives input images of size 224 x 224 x 1\n",
        "2. the input data goes through a flattening layer\n",
        "3. the flatten input goes through three connected layers, followed by a dropout layer\n",
        "\n",
        "For the dropout layer, we set the probability of dropping input units during training to 0.3.\n",
        "\n",
        "We will implement this architecture using TensorFlow Keras API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlpzia0vkXDb"
      },
      "source": [
        "``Build model``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YkKA1tPkXDb"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# first layer: input\n",
        "input1 = tf.keras.layers.Input(shape=(224, 224, 1), name='Input')\n",
        "\n",
        "# second layer: flatten (transforms the format of the images (to a 1D array of 224*224 = 50176 pixels)\n",
        "# this layer has no params or weights to learn, it only reformats data\n",
        "input2 = tf.keras.layers.Flatten(name='Flatten')(input1)\n",
        "\n",
        "# third layer: dense (i.e., the nodes are fully connected)\n",
        "# this layer has 12544 nodes (112 x 112 image), and it uses the activation function Rectified Linear Unit\n",
        "input3 = tf.keras.layers.Dense(units=12544, activation='relu', name='fc_1')(input2)\n",
        "\n",
        "# forth layer: dense (i.e., the nodes are fully connected)\n",
        "# this layer has 3136 nodes (56 x 56 image), and it uses the activation function Rectified Linear Unit\n",
        "input4 = tf.keras.layers.Dense(units=3136, activation='relu', name='fc_2')(input3)\n",
        "\n",
        "\n",
        "# fifth layer: dense (i.e., the nodes are fully connected)\n",
        "# this layer has 784 nodes (28 x 28), and it uses the activation function Rectified Linear Unit\n",
        "input5 = tf.keras.layers.Dense(units=784, activation='relu', name='fc_3')(input4)\n",
        "\n",
        "\n",
        "# sixth layer: dropout\n",
        "# makes it so some of the nodes in a given layer donâ€™t pass on their information to the next layer.\n",
        "# This helps with computation time (less parameters) and with overfitting.\n",
        "# each node in the third layer has a 0.3 probability of being dropped from the computation of the activations of the next layer.\n",
        "input6 = tf.keras.layers.Dropout(rate=0.3, name='Dropout')(input5)\n",
        "\n",
        "#the sixth (and last) layer, which is the output layer\n",
        "# this last layer sets the activation function to \"None\" in order to output the logits\n",
        "# logits = natural logarithm of the odds ratio\n",
        "# note that passing activation = \"sigmoid\" will return class memembership probabilities but\n",
        "# in TensorFlow logits are prefered for numerical stability\n",
        "# set units=1 to get a single output unit (remember it's a binary classification problem)\n",
        "out = tf.keras.layers.Dense(units=1, activation=None, name='fc_4')(input6)\n",
        "\n",
        "# instantiate the model\n",
        "model = tf.keras.models.Model(inputs=input1, outputs=out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr50ZfttkXDb"
      },
      "source": [
        "``Compile model``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7_Dcj10kXDb"
      },
      "source": [
        "The next step is to compile the model. We have to decide on the type of optimizer, loss function, and metrics to compute.\n",
        "\n",
        "We will use the Adam optimizer, the most popular gradient-based optimization algorithm. There are a few other choices, and you can read more [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers). The loss (cost) function suitable for our binary classification model is [binary_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses). We will compute model accuracy on the training, validation and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gT0kxNJkXDc"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              # ^ because our last layer does not apply sigmoid, by specifying from logits=True, the loss function applies the sigmoid function internally\n",
        "              # ^ it's more efficient than doing it manually\n",
        "              metrics=['accuracy']) # you can add other metrics here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LbWeFEGkXDc"
      },
      "source": [
        "``Fit model``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg-EtWwJkXDc"
      },
      "source": [
        "Finally, we will fit the model with 10 epochs on the train set and validate on the validation set. The performance depends on the current starter hyperparameters such as learning rate and choice of optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOK5whtIkXDc"
      },
      "outputs": [],
      "source": [
        "# set random seed to get reproductible results\n",
        "# neural network algorithms are stochastic (e.g., due to random weight initialization); setting a random seed helps to get more stable results after each run\n",
        "# however, best way to deal with randomness is to repeat your experiment many times (30+) and use statistics to summarize the performance of the model\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    validation_data=(X_val, y_val)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjD4vQSqkXDc"
      },
      "source": [
        "Next let's plot loss and accuracy for training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u46pd71EkXDc"
      },
      "outputs": [],
      "source": [
        "hist = history.history\n",
        "x_arr = np.arange(len(hist['loss'])) + 1\n",
        "\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "ax = fig.add_subplot(1, 2, 1)\n",
        "ax.plot(x_arr, hist['loss'], '-o', label='Train loss')\n",
        "ax.plot(x_arr, hist['val_loss'], '--<', label='Validation loss')\n",
        "ax.legend(fontsize=15)\n",
        "ax.set_xlabel('Epoch', size=15)\n",
        "ax.set_ylabel('Loss', size=15)\n",
        "\n",
        "ax = fig.add_subplot(1, 2, 2)\n",
        "ax.plot(x_arr, hist['accuracy'], '-o', label='Train acc.')\n",
        "ax.plot(x_arr, hist['val_accuracy'], '--<', label='Validation acc.')\n",
        "ax.legend(fontsize=15)\n",
        "ax.set_xlabel('Epoch', size=15)\n",
        "ax.set_ylabel('Accuracy', size=15)\n",
        "ax.set_ylim(0,1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLz40irQkXDc"
      },
      "source": [
        "## Show what the NN model is learning after each layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upTRS7ufkXDd"
      },
      "source": [
        "We will pick one example from our training data to visualize our NN model's learning after each layer. Below we print the original image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZv1GivHkXDd"
      },
      "outputs": [],
      "source": [
        "img_idx= 14\n",
        "img_tensor = np.expand_dims(X_val[img_idx], axis = 0);\n",
        "\n",
        "# Print image tensor shape\n",
        "print('Shape of image:', img_tensor.shape);\n",
        "\n",
        "# Print image (should be of shape 224 X 224)\n",
        "plt.imshow(img_tensor[0]);\n",
        "plt.title('label:' + str(y_val[img_idx]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDrvH2YEkXDd"
      },
      "source": [
        "Next we print what the model learns after each layer. It's important to pay attention to the shape of the output image to understand what each layer does to the original input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co2i8rtUkXDd"
      },
      "source": [
        "``print layers``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWg2ApFxkXDd"
      },
      "outputs": [],
      "source": [
        "layers = [layer.output for layer in model.layers[:6]]\n",
        "layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeMcMvXpkXDd"
      },
      "source": [
        "``activation model``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF2DADnFkXDd"
      },
      "outputs": [],
      "source": [
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layers)\n",
        "activation_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5hsRujxkXDd"
      },
      "source": [
        "``data frames of learned activations for each layer``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmQ6WZFokXDd"
      },
      "outputs": [],
      "source": [
        "activations = activation_model.predict(img_tensor)\n",
        "\n",
        "## print to see what are the learned activations ##\n",
        "##################################################\n",
        "# original example\n",
        "print('Feature values original example:')\n",
        "display(pd.DataFrame(tf.squeeze(X_val[img_idx]).numpy()).head(3))\n",
        "\n",
        "print('Learned activations first layer (should be the same as the original example, nothing to learn yet):')\n",
        "# first layer (just flattening the 224x224 images to a 50176 array)\n",
        "display(pd.DataFrame(activations[1].reshape(224,224)).head(3))\n",
        "\n",
        "print('Learned activations second layer:')\n",
        "# second layer (activate 12544 neurons (equivalent to 56 x 56 images)\n",
        "display(pd.DataFrame(activations[2].reshape(112,112)).head(3))\n",
        "\n",
        "print('Learned activations third layer:')\n",
        "# second layer (activate 3136 neurons (equivalent to 56 x 56 images)\n",
        "display(pd.DataFrame(activations[3].reshape(56,56)).head(3))\n",
        "\n",
        "print('Learned activations forth layer:')\n",
        "# second layer (activate 784 neurons (equivalent to 28 x 28 images)\n",
        "display(pd.DataFrame(activations[4].reshape(28,28)).head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb-WP5hWkXDe"
      },
      "source": [
        "``images of learned activations for each layer``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRJCo-TwkXDe"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 5, sharey=False, figsize=(15, 25))\n",
        "\n",
        "\n",
        "for idx, ax in enumerate(axes.flatten()):\n",
        "    # save activation and shape\n",
        "    activation = activations[idx+1]\n",
        "    shape = int(np.sqrt(activation.shape)[1]), int(np.sqrt(activation.shape)[1])\n",
        "\n",
        "    # plot activation\n",
        "    ax.imshow(activation.reshape(shape))\n",
        "    ax.set_xlim(0, shape[0])\n",
        "    ax.set_ylim(0, shape[0])\n",
        "\n",
        "    if idx==0:\n",
        "        ax.set_title('label: ' + str(y_val[img_idx]) +\n",
        "                     '\\nlayer: ' + layers[idx+1].name.split('/')[0] +\n",
        "                     '\\n image size: ' +  str(activation.reshape(shape).shape))\n",
        "    else:\n",
        "        ax.set_title('\\nlayer: ' + layers[idx+1].name.split('/')[0] +\n",
        "                     '\\n image size: ' +  str(activation.reshape(shape).shape))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjEnaq2NkXDe"
      },
      "source": [
        "``Can we do better in terms of model interpretability? YES!``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_P3JUGwkXDe"
      },
      "source": [
        "<span style=\"color:chocolate\">Explainable AI/ML using Shap</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2JkDjpwkXDe"
      },
      "source": [
        "SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see [here](https://github.com/slundberg/shap#citations) and [here](https://shap.readthedocs.io/en/latest/overviews.html) and [here](https://databricks.com/fr/wp-content/uploads/2019/10/Introduction-to-Neural-Networks-MLflow-and-SHAP.html))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cukQTUckXDe"
      },
      "source": [
        "We will use SHAP to explain the predictions made by our model on the validation image at index==img_idx using GradientExplainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-7PSXpIkXDf"
      },
      "outputs": [],
      "source": [
        "# define the explainer\n",
        "explainer = shap.GradientExplainer(model, X_train.numpy())\n",
        "\n",
        "# we explain the model's predictions on the validation image at index==img_idx\n",
        "shap_values = explainer.shap_values(X_val[img_idx:img_idx+1].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR51nvA-kXDf"
      },
      "outputs": [],
      "source": [
        "# plot the explanations for all classes (well, binary outcome)\n",
        "shap.image_plot(\n",
        "    [shap_values[i][0] for i in range(1)],\n",
        "    np.squeeze(X_val[img_idx]),\n",
        "    show=False\n",
        ")\n",
        "\n",
        "# compute predicted label for X_val[img_idx]\n",
        "y_val_pred_img_idx = np.where(tf.sigmoid(model.predict(X_val)[img_idx])>=0.5, 1, 0)\n",
        "\n",
        "# add title\n",
        "plt.suptitle(\n",
        "    'True label: ' + str(y_val[img_idx]) +\n",
        "    '\\n Predicted label: ' + str(y_val_pred_img_idx[0]));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-UiYJStkXDf"
      },
      "source": [
        "The plot above shows the SHAP explanations for the X_val[img_idx] example. On the LHS is the original image. Red pixels increase the model's output. Blue pixels decrease the model's output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx_JI11AkXDf"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHE8Sp92kXDf"
      },
      "source": [
        "Evaluation is one of the most important parts of machine learning as it helps us determine how good our trained model is in predicting unseen data.\n",
        "\n",
        "Notice that (`X_test`, and `y_test`) were not used in the training part. It would be very bad practice to evaluate the model on the test set, and then return and update the model based on those results (then the test set is acting like just another validation set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSnAKYMbkXDf"
      },
      "source": [
        "We will now use our test data to evaluate the performance (accuracy) of our NN model on unseen data. Note that accuracy is the default metric if one compiles the model with the accuracy metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APYUn4DKkXDf"
      },
      "outputs": [],
      "source": [
        "test_results = model.evaluate(X_test, y_test)\n",
        "print('\\nTest Acc. {:.2f}%'.format(test_results[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MonouiTkXDf"
      },
      "source": [
        "`get predictions results in the form of class-membership probabilities`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-wbzLtJkXDf"
      },
      "source": [
        "In the following figure, you can see all the images in the test data along with their ground truth (GT) labels and the predicted probabiliy that they belong to class 1, 'Refer'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkipbaOykXDf"
      },
      "outputs": [],
      "source": [
        "# transform logits to probabilities\n",
        "pred_logits = model.predict(X_test)\n",
        "probas = tf.sigmoid(pred_logits)\n",
        "probas = probas.numpy().flatten()*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FplexyeUkXDf"
      },
      "outputs": [],
      "source": [
        "# plot test data and associated predicred\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "for j, example in enumerate(X_test):\n",
        "    ax = fig.add_subplot(8,4, j+1)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.imshow(array_to_img(example))\n",
        "    if y_test[j]==0:\n",
        "        label='no refer'\n",
        "    else:\n",
        "        label='refer'\n",
        "\n",
        "    ax.text(\n",
        "        0.5, -0.15,\n",
        "        'GT: {:s}\\nPr(refer)={:.0f}%'.format(label, probas[j]),\n",
        "        size=16,\n",
        "        color='grey',\n",
        "        horizontalalignment='center',\n",
        "        verticalalignment='center',\n",
        "        transform=ax.transAxes)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}