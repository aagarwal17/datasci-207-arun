{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67abc16d",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72deb5",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\"> Submission requirements </span>\n",
    "\n",
    "Your work will not be graded if your notebook doesn't include output. In other words, <span style=\"color:red\"> make sure to rerun your notebook before submitting to Gradescope </span> (Note: if you are using Google Colab: go to Edit > Notebook Settings  and uncheck Omit code cell output when saving this notebook, otherwise the output is not printed).\n",
    "\n",
    "Additional points may be deducted if these requirements are not met:\n",
    "    \n",
    "* Comment your code;\n",
    "* Each graph should have a title, labels for each axis, and (if needed) a legend. Each graph should be understandable on its own;\n",
    "* Try and minimize the use of the global namespace (meaning, keep things inside functions).\n",
    "\n",
    "Additional notes:\n",
    "\n",
    "* Please note that in this assignment, students are expected to work independently. As a result, no two solutions should look identical in terms of coding;\n",
    "* You may import any libraries you need to complete the assignment. However, you must implement the model using TensorFlow (do not use PyTorch);\n",
    "* Follow the same steps/idea as in Assignment 4; the difference here is that you extend a logistic model to more than two classes;\n",
    "* <span style=\"color:chocolate\"> Focus on the execution of the task rather than model performance </span> (this is how the TA will grade your work);\n",
    "* Even though the prediction performance for your chosen outcome is low, it doesn't necessarily mean there is something wrong with your implementation. It could also be that the data is not supportive enough for your prediction task... again, focus on the learning opportunity and not the numbers you get;\n",
    "* Your instructional team has extensive experience developing and running ML models. Often, we encounter situations where a model doesn't perform well on a predictive task. This can happen due to the nature of the data or the need for significant tweaking of variables to achieve good results;\n",
    "* Do not spend significantly more time on this task than you did on Assignment 4, unless you wish to experiment and learn more.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6df70",
   "metadata": {},
   "source": [
    "``Objective``\n",
    "* Perform multiclass classification using logistic regression. <span style=\"color:chocolate\"> You will choose the outcome of interest. </span>\n",
    "\n",
    "``Motivation``\n",
    "* Chocolate is one of the most popular candies in the world. Each year, residents of the United States collectively eat more than 2.8 billions pounds (Source: Kaggle). However, not all chocolate bars are created equal! In this assignment, you will have the opportunity to delve into the world of chocolate by choosing your own machine learning task. \n",
    "\n",
    "\n",
    "``Data``\n",
    "\n",
    "* The [Chocolate Bar dataset](https://www.kaggle.com/datasets/rtatman/chocolate-bar-ratings) contains expert ratings of 1,795 individual chocolate bars, along with information on their regional origin, percentage of cocoa, the variety of chocolate bean used and where the beans were grown (Source: Kaggle)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999fc96",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1be241",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy.linalg as nla\n",
    "import pandas as pd\n",
    "import re\n",
    "import six\n",
    "from os.path import join\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_tuner import HyperParameters\n",
    "\n",
    "# feel free to import other libraries as needed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f44e88f",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd91c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    ''''''\n",
    "    # Read data\n",
    "    df = pd.read_csv(\n",
    "        \"https://download.mlcc.google.com/mledu-datasets/flavors_of_cacao.csv\",\n",
    "        sep=\",\",\n",
    "        encoding='latin-1'\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a182b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    ''''''\n",
    "    # Set the output display to have one digit for decimal places and limit it to\n",
    "    # printing 15 rows.\n",
    "    pd.options.display.float_format = '{:.2f}'.format\n",
    "    pd.options.display.max_rows = 15\n",
    "    \n",
    "    # Rename the columns.\n",
    "    df.columns = [\n",
    "        'maker', 'specific_origin', 'reference_number',\n",
    "        'review_date', 'cocoa_percent', 'maker_location',\n",
    "        'rating', 'bean_type', 'broad_origin'\n",
    "    ]\n",
    "\n",
    "    # df.dtypes\n",
    "\n",
    "    # Replace empty/null values with \"Blend\"\n",
    "    df['bean_type'] = df['bean_type'].fillna('Blend')\n",
    "\n",
    "    # Cast bean_type to string to remove leading 'u'\n",
    "    df['bean_type'] = df['bean_type'].astype(str)\n",
    "    df['cocoa_percent'] = df['cocoa_percent'].str.strip('%')\n",
    "    df['cocoa_percent'] = pd.to_numeric(df['cocoa_percent'])\n",
    "\n",
    "    # Correct spelling mistakes, and replace city with country name\n",
    "    df['maker_location'] = df['maker_location']\\\n",
    "    .str.replace('Amsterdam', 'Holland')\\\n",
    "    .str.replace('U.K.', 'England')\\\n",
    "    .str.replace('Niacragua', 'Nicaragua')\\\n",
    "    .str.replace('Domincan Republic', 'Dominican Republic')\n",
    "\n",
    "    # Adding this so that Holland and Netherlands map to the same country.\n",
    "    df['maker_location'] = df['maker_location']\\\n",
    "    .str.replace('Holland', 'Netherlands')\n",
    "\n",
    "    def cleanup_spelling_abbrev(text):\n",
    "        replacements = [\n",
    "            ['-', ', '], ['/ ', ', '], ['/', ', '], ['\\(', ', '], [' and', ', '], [' &', ', '], ['\\)', ''],\n",
    "            ['Dom Rep|DR|Domin Rep|Dominican Rep,|Domincan Republic', 'Dominican Republic'],\n",
    "            ['Mad,|Mad$', 'Madagascar, '],\n",
    "            ['PNG', 'Papua New Guinea, '],\n",
    "            ['Guat,|Guat$', 'Guatemala, '],\n",
    "            ['Ven,|Ven$|Venez,|Venez$', 'Venezuela, '],\n",
    "            ['Ecu,|Ecu$|Ecuad,|Ecuad$', 'Ecuador, '],\n",
    "            ['Nic,|Nic$', 'Nicaragua, '],\n",
    "            ['Cost Rica', 'Costa Rica'],\n",
    "            ['Mex,|Mex$', 'Mexico, '],\n",
    "            ['Jam,|Jam$', 'Jamaica, '],\n",
    "            ['Haw,|Haw$', 'Hawaii, '],\n",
    "            ['Gre,|Gre$', 'Grenada, '],\n",
    "            ['Tri,|Tri$', 'Trinidad, '],\n",
    "            ['C Am', 'Central America'],\n",
    "            ['S America', 'South America'],\n",
    "            [', $', ''], [',  ', ', '], [', ,', ', '], ['\\xa0', ' '],[',\\s+', ','],\n",
    "            [' Bali', ',Bali']\n",
    "        ]\n",
    "        for i, j in replacements:\n",
    "            text = re.sub(i, j, text)\n",
    "        return text\n",
    "\n",
    "    df['specific_origin'] = df['specific_origin'].str.replace('.', '').apply(cleanup_spelling_abbrev)\n",
    "\n",
    "    # Cast specific_origin to string\n",
    "    df['specific_origin'] = df['specific_origin'].astype(str)\n",
    "\n",
    "    # Replace null-valued fields with the same value as for specific_origin\n",
    "    df['broad_origin'] = df['broad_origin'].fillna(df['specific_origin'])\n",
    "\n",
    "    # Clean up spelling mistakes and deal with abbreviations\n",
    "    df['broad_origin'] = df['broad_origin'].str.replace('.', '').apply(cleanup_spelling_abbrev)\n",
    "\n",
    "    # Change 'Trinitario, Criollo' to \"Criollo, Trinitario\"\n",
    "    # Check with df['bean_type'].unique()\n",
    "    df.loc[df['bean_type'].isin(['Trinitario, Criollo']),'bean_type'] = \"Criollo, Trinitario\"\n",
    "    # Confirm with df[df['bean_type'].isin(['Trinitario, Criollo'])]\n",
    "\n",
    "    # Fix chocolate maker names\n",
    "    df.loc[df['maker']=='Shattel','maker'] = 'Shattell'\n",
    "    df['maker'] = df['maker'].str.replace(u'Na\\xef\\xbf\\xbdve','Naive')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389425c1",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b454bc04",
   "metadata": {},
   "source": [
    "First, we'll initiate the process of discovering the chocolate world by loading the data. Then, to assist with this assignment, we'll start by tidying up the data a little bit. This involves renaming columns and conducting some string preprocessing tasks, which will be handled by the <span style=\"color:chocolate\">clean_data()</span> function mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48857a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data(read_data())\n",
    "print('Shape of data', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3be38f",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 1:</span> Getting to know the data (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed93cfb1",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "    \n",
    "1. How many columns does the dataset contain?\n",
    "2. How many rows are there in the dataset?\n",
    "3. What are the column names?\n",
    "4. List the number of unique values for each column in the data;\n",
    "5. What are the unique cocoa_percent values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa25981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb7815",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 2:</span> Choosing the prediction task (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858aaf43",
   "metadata": {},
   "source": [
    "Now that you’ve explored the data, choose a multiclass outcome (anything except \"ratings\") that you’re interested in predicting. Note: The outcome should have <span style=\"color:chocolate\">at least 3 classes</span>!\n",
    "\n",
    "If your chosen outcome variable requires preprocessing, go ahead and handle that below. For instance, you might choose to predict \"cocoa_percent\". Discretizing it into \"0=low,\" \"1=medium,\" and \"2=high\" makes it easier to work with/interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c9b3e0",
   "metadata": {},
   "source": [
    "Your answer here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf0526-3e99-4a59-ade9-d5935f41a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50837672",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6558b",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 3:</span> Prepare data for modeling (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5bd4b",
   "metadata": {},
   "source": [
    "Following the format of previous assignments, adhere to the following steps as a minimum:\n",
    "\n",
    "1. Shuffle the dataset;\n",
    "2. Create training, validation, and test datasets using a 60/20/20 split;\n",
    "3. Identify the features of interest;\n",
    "4. Perform necessary cleaning and standarization on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd4232",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7896a1",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 4:</span> Plots (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9182a26",
   "metadata": {},
   "source": [
    "In line with the structure of previous assignments, execute the following steps:\n",
    "\n",
    "1. Generate a minimum of 4 plots to investigate features and outcome within the training dataset;\n",
    "2. Ensure that each plot includes clear axis labels and titles;\n",
    "3. Provide commentary on the insights learned from your visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e2064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9674f",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcc92d",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 5:</span> Baseline model (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948692c",
   "metadata": {},
   "source": [
    "When dealing with classification problems, a simple baseline is to select the *majority* class (the most common label in the training set) and use it as the prediction for all inputs.\n",
    "\n",
    "1. Implement this baseline and report the accuracy metric on the train data;\n",
    "\n",
    "2. Implement a function that computes the Log Loss (cross-entropy loss) metric and use it to evaluate this baseline on both the train and validation data. Note: reflect on what you know about the original distribution of classes in your training data (Hint: see Assignment 4 - Exercise 8 and ``Module Demos/05 Multiclass Logistic Regression.ipynb`` in bCourses for an example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536702e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a8365",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 6:</span> Improvement over baseline with Tensorflow (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311dd3ad",
   "metadata": {},
   "source": [
    "Use TensorFlow (TF) to train a multiclass logistic regression model much like you did in Assignment 4. The goal here is to build a ML model to improve over the baseline classifier. You have the flexibility to choose which features to include.\n",
    "\n",
    "With this in mind, complete the following tasks:\n",
    "\n",
    "1. Build and compile a multiclass classification TF model (call it model_tf). Hint: the activation function, the loss, and the evaluation metric are different compared to the binary logistic regression (see ``Module Demos/05 Multiclass Logistic Regression.ipynb`` in bCourses for an example). Set learning_rate = 0.0001 and optimizer = SGD.\n",
    "2. Train model_tf using the training dataset and pass the validation data for validation. Set num_epochs = 10 and batch_size = 32.\n",
    "3. Generate a plot (for the training and validation data) with the loss values on the y-axis and the epoch number on the x-axis for visualization. Make sure to include axes name and title.\n",
    "\n",
    "If instructions for any other hyperparameters are not provided here, you are free to select your own or use the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf162e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7872617",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd6bec",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 7:</span> Choosing hyperparameters (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6c715",
   "metadata": {},
   "source": [
    "1. Fine-tune the **learning rate**, **number of epochs**, and **batch size** hyperparameters of *model_tf* to determine the setup that yields the most optimal generalization performance. Feel free to explore various values for these hyperparameters. Hint: you can manually test different hyperparameter values or you can use the [Keras Tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner). \n",
    "\n",
    "After identifying your preferred model configuration, print the following information:\n",
    "\n",
    "2. The first five learned parameters of the model (this should include the bias term);\n",
    "3. The loss at the final epoch on both the training and validation datasets;\n",
    "4. The percentage difference between the losses observed on the training and validation datasets.\n",
    "5. Compare the training/validation loss of the TensorFlow model (model_tf) with the baseline model's loss. Does the TensorFlow model demonstrate an improvement over the baseline model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb087a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8b941b",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 6: Evaluation and generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1640c795",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 8:</span> Compute metrics (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78fc49",
   "metadata": {},
   "source": [
    "Now that you've determined the optimal set of hyperparameters, it's time to evaluate your optimized model on the test data to gauge its performance in real-world scenarios, commonly known as inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a452f0",
   "metadata": {},
   "source": [
    "1. Calculate aggregate accuracy on both train and test datasets. Note: you will need to convert the vector of predicted probabilities to a class label using the argmax operation. Hint: You can utilize the <span style=\"color:chocolate\">model.predict()</span> method provided by tf.keras. and the <span style=\"color:chocolate\">np.max()</span> method available in NumPy.\n",
    "\n",
    "2. Does the model demonstrate strong aggregate generalization capabilities? Provide an explanation based on your accuracy observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a896a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46219440",
   "metadata": {},
   "source": [
    "### <span style=\"color:chocolate\">Exercise 9:</span> Additional metrics (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd95a071",
   "metadata": {},
   "source": [
    "Using the test dataset:\n",
    "\n",
    "1. Plot the confusion matrix. Identify which class the model confuses the most.\n",
    "\n",
    "2. Determine which class has the lowest precision. What is the precision? Which class is the largest source of false positives?\n",
    "\n",
    "3. Determine which class has the lowest recall. What is the recall? Which class is the largest source of false negatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42733dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599dc56",
   "metadata": {},
   "source": [
    "----\n",
    "#### <span style=\"color:chocolate\">Additional practice question</span> (not graded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e04ef4",
   "metadata": {},
   "source": [
    "Following the approach in Assignment 4 - Exercise 12, evaluate whether your model shows any signs of unfairness. Explain your findings and propose suggestions for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f2de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
