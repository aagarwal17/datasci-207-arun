{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKsRDH5ZUdfasdv"
      },
      "source": [
        "# Assignment 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L2W54xftRsf"
      },
      "source": [
        "### <span style=\"color:chocolate\"> Submission requirements </span>\n",
        "\n",
        "Your work will not be graded if your notebook doesn't include output. In other words, <span style=\"color:red\"> make sure to rerun your notebook before submitting to Gradescope </span> (Note: if you are using Google Colab: go to Edit > Notebook Settings  and uncheck Omit code cell output when saving this notebook, otherwise the output is not printed).\n",
        "\n",
        "Additional points may be deducted if these requirements are not met:\n",
        "\n",
        "    \n",
        "* Comment your code;\n",
        "* Each graph should have a title, labels for each axis, and (if needed) a legend. Each graph should be understandable on its own;\n",
        "* Try and minimize the use of the global namespace (meaning, keep things inside functions).\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7isFYawtRsg"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X58hOMTUH-w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style=\"darkgrid\")  # default style\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import metrics\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "tf.get_logger().setLevel('INFO')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tow9JWNQtRsi"
      },
      "source": [
        "---\n",
        "### Step 1: Data ingestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHLcriKWLRe4"
      },
      "source": [
        "You'll train a binary classifier using the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. This consists of 70,000 grayscale images (28x28). Each image is associated with 1 of 10 classes. The dataset was split by the creators; there are 60,000 training images and 10,000 test images. Note also that Tensorflow includes a growing [library of datasets](https://www.tensorflow.org/datasets/catalog/overview) and makes it easy to load them in numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv4_ZfxPtRsi"
      },
      "outputs": [],
      "source": [
        "# Load the Fashion MNIST dataset.\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7170TMVZtRsj"
      },
      "source": [
        "---\n",
        "### Step 2: Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_IsKbHmtRsj"
      },
      "source": [
        "Exploratory Data Analysis (EDA) and Data Preprocessing are often iterative processes that involve going back and forth to refine and improve the quality of data analysis and preparation. However, the specific order can vary depending on the project's requirements. In some cases, starting with EDA, as you see in this assignment, could be more useful, but there is no rigid rule dictating the sequence in all situations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmfOampptRsj"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 1:</span> Getting to know your data (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_auto_data_set_code"
      },
      "source": [
        "Complete the following tasks:\n",
        "\n",
        "1. Print the shapes and types of (X_train, Y_train) and (X_test, Y_test). Interpret the shapes (i.e., what do the numbers represent?). Hint: For types use the <span style=\"color:chocolate\">type()</span> function.\n",
        "2. Define a list of strings of class names corresponding to each class in (Y_train, Y_test). Call this list label_names. Hint: Refer to the Fashion MNIST documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUXF1apotRsj"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# 1:\n",
        "print(\"X_train shape:\", X_train.shape, \", type:\", type(X_train))\n",
        "print(\"Interpretation: X_train contains 60,000 training images of size 28x28 pixels.\")\n",
        "print(\"Y_train shape:\", Y_train.shape, \", type:\", type(Y_train))\n",
        "print(\"Interpretation: Y_train contains 60,000 labels, one for each training image.\")\n",
        "print(\"X_test shape:\", X_test.shape, \", type:\", type(X_test))\n",
        "print(\"Interpretation: X_test contains 10,000 test images of size 28x28 pixels.\")\n",
        "print(\"Y_test shape:\", Y_test.shape, \", type:\", type(Y_test))\n",
        "print(\"Interpretation: Y_test contains 10,000 labels, one for each test image.\")\n",
        "\n",
        "# 2:\n",
        "label_names = [\n",
        "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
        "]\n",
        "print(\"Class names defined as:\", label_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRM2JPHctRsk"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 2:</span> Getting to know your data - cont'd (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWitcNIztRsk"
      },
      "source": [
        "Fashion MNIST images have one of 10 possible labels (shown above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eu-PYa3tRsk"
      },
      "source": [
        "Complete the following tasks:\n",
        "\n",
        "1. Display the first 5 images in X_train for each class in Y_train, arranged in a 10x5 grid. Use the label_names list defined above;\n",
        "2. Determine the minimum and maximum pixel values for images in the X_train dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_bMfi1ZtRsk"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# 1:\n",
        "plt.figure(figsize=(15, 15))\n",
        "for class_index in range(10):\n",
        "    # Extract indices of the first 5 images for the current class\n",
        "    class_indices = np.where(Y_train == class_index)[0][:5]\n",
        "    for i, image_index in enumerate(class_indices):\n",
        "        plt.subplot(10, 5, class_index * 5 + i + 1)\n",
        "        plt.imshow(X_train[image_index], cmap=\"gray\")\n",
        "        plt.axis('off')\n",
        "        plt.title(label_names[class_index])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2:\n",
        "min_pixel_value = np.min(X_train)\n",
        "max_pixel_value = np.max(X_train)\n",
        "\n",
        "print(\"Minimum pixel value in X_train:\", min_pixel_value)\n",
        "print(\"Maximum pixel value in X_train:\", max_pixel_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZpFxxStzSrP"
      },
      "source": [
        "---\n",
        "### Step 3: Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkzgL73mtRsk"
      },
      "source": [
        "This step is essential for preparing this image data in a format that is suitable for ML algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk7cw-I6tRsk"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 3:</span> Feature preprocessing (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K2FvYIttRsl"
      },
      "source": [
        "In the previous lab, the input data had just a few features. Here, we treat **every pixel value as a separate feature**, so each input example has 28x28 (784) features!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VcSgaYNtRsl"
      },
      "source": [
        "In this exercise, you'll perform the following tasks:\n",
        "\n",
        "1. Normalize the pixel values in both X_train and X_test data so they range between 0 and 1;\n",
        "2. For each image in X_train and X_test, flatten the 2-D 28x28 pixel array to a 1-D array of size 784. Hint: use the <span style=\"color:chocolate\">reshape()</span> method available in NumPy. Note that by doing so you will overwrite the original arrays;\n",
        "3. Pint the shape of X_train and X_test arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-U0FGK0tRsl"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# 1:\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# 2:\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# 3:\n",
        "print(\"X_train shape after preprocessing:\", X_train.shape)\n",
        "print(\"X_test shape after preprocessing:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1IQ4xlGtRsl"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 4:</span> Label preprocessing (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uib_4YQwtRsl"
      },
      "source": [
        "This assignment involves binary classification. Specifically, the objective is to predict whether an image belongs to the sneaker class (class 7) or not.\n",
        "\n",
        "Therefore, write code so that for each example in (Y_train, Y_test), the outcome variable is represented as follows:\n",
        "* $y=1$, for sneaker class (positive examples), and\n",
        "* $y=0$, for non-sneaker class (negative examples).\n",
        "\n",
        "Note: To avoid \"ValueError: assignment destination is read-only\", first create a copy of the (Y_train, Y_test) data and call the resulting arrays (Y_train, Y_test). Then overwrite the (Y_train, Y_test) arrays to create binary outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6C7RcxPtRsl"
      },
      "outputs": [],
      "source": [
        "# Make copies of the original dataset for binary classification task.\n",
        "Y_train = np.copy(Y_train)\n",
        "Y_test = np.copy(Y_test)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# This will make it so it's 1 for sneaker class, 0 otherwise\n",
        "Y_train = (Y_train == 7).astype(int)\n",
        "Y_test = (Y_test == 7).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ63zJW4tRsl"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 5:</span> Data splits (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0KEMhLttRsl"
      },
      "source": [
        "Using the <span style=\"color:chocolate\">train_test_split()</span> method available in scikit-learn:\n",
        "1. Retain 20% from the training data for validation purposes. Set random state to 1234. All the other arguments of the method are set to default values. Name the resulting dataframes as follows: X_train_mini, X_val, Y_train_mini, Y_val.\n",
        "2. Print the shape of each array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djDMkJPQtRsm"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# 1:\n",
        "X_train_mini, X_val, Y_train_mini, Y_val = train_test_split(\n",
        "    X_train, Y_train, test_size=0.2, random_state=1234\n",
        ")\n",
        "\n",
        "# 2:\n",
        "print(\"X_train_mini shape:\", X_train_mini.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"Y_train_mini shape:\", Y_train_mini.shape)\n",
        "print(\"Y_val shape:\", Y_val.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M_si1vRtRsm"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 6:</span> Data shuffling (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC5bmzCrtRsm"
      },
      "source": [
        "Since you'll be using Batch Gradient Descent (BGD) for training, it is important that **each batch is a random sample of the data** so that the gradient computed is representative.\n",
        "\n",
        "1. Use [integer array indexing](https://numpy.org/doc/stable/reference/arrays.indexing.html#integer-array-indexing) to re-order (X_train_mini, Y_train_mini) using a list of shuffled indices. In doing so, you will overwrite the arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5bc4j49tRsm"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "# YOUR CODE HERE\n",
        "shuffled_indices = np.random.permutation(len(X_train_mini))\n",
        "X_train_mini = X_train_mini[shuffled_indices]\n",
        "Y_train_mini = Y_train_mini[shuffled_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXXdOBoZtRsm"
      },
      "source": [
        "---\n",
        "### Step 4: Exploratory Data Analysis (EDA) - cont'd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St-ZlaREtRsm"
      },
      "source": [
        "Before delving into model training, let's further explore the raw feature values by comparing sneaker and non-sneaker training images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBIAGffrtRsm"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 7:</span> Pixel distributions (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MICdCkAQtRsn"
      },
      "source": [
        "1. Identify all sneaker images in X_train_mini and calculate the mean pixel value for each sneaker image. Visualize these pixel values using a histogram. Print the mean pixel value across all sneaker images.\n",
        "2. Identify all non-sneaker images in X_train_mini and calculate the mean pixel value for each non-sneaker image. Visualize these pixel values using a histogram. Print the mean pixel value across all non-sneaker images.\n",
        "3. Based on the histogram results, assess whether there is any evidence suggesting that pixel values can be utilized to distinguish between sneaker and non-sneaker images. Justify your response.\n",
        "\n",
        "Notes: Make sure to provide a descriptive title and axis labels for each histogran. Make sure you utilize Y_train_mini to locate the sneaker and non-sneaker class."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Identifying sneaker images and getting means\n",
        "sneaker_indices = np.where(Y_train_mini == 1)[0]\n",
        "sneaker_means = X_train_mini[sneaker_indices].mean(axis=1)\n",
        "\n",
        "# Identifying non-sneaker images and getting means\n",
        "non_sneaker_indices = np.where(Y_train_mini == 0)[0]\n",
        "non_sneaker_means = X_train_mini[non_sneaker_indices].mean(axis=1)\n",
        "\n",
        "# 1:\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(sneaker_means, bins=30, alpha=0.7, color='blue', label=\"Sneaker Images\")\n",
        "plt.title(\"Histogram of Mean Pixel Values for Sneaker Images\")\n",
        "plt.xlabel(\"Mean Pixel Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "\n",
        "# 2:\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(non_sneaker_means, bins=30, alpha=0.7, color='green', label=\"Non-Sneaker Images\")\n",
        "plt.title(\"Histogram of Mean Pixel Values for Non-Sneaker Images\")\n",
        "plt.xlabel(\"Mean Pixel Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "sneaker_mean_pixel_value = sneaker_means.mean()\n",
        "print(\"Mean pixel value across all sneaker images:\", sneaker_mean_pixel_value)\n",
        "non_sneaker_mean_pixel_value = non_sneaker_means.mean()\n",
        "print(\"Mean pixel value across all non-sneaker images:\", non_sneaker_mean_pixel_value)\n",
        "\n",
        "# 3:\n",
        "print(\"Based on the printed Histograms above, there is a clear difference in the mean pixel values for sneaker and non-sneaker images.\\nThe distribution of mean pixel values for non-sneaker images is much wider than is the distribution for sneaker images.\\nThis suggests that pixel values might be useful for distinguishing between the two classes.\")\n"
      ],
      "metadata": {
        "id": "Dfw0hfOe2uAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeIXM05ktRsn"
      },
      "source": [
        "---\n",
        "### Step 4: Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2c3DMbqtRsn"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 8:</span> Baseline model (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGgQ-ASYiHbK"
      },
      "source": [
        "When dealing with classification problems, a simple baseline is to select the *majority* class (the most common label in the training set) and use it as the prediction for all inputs.\n",
        "\n",
        "With this information in mind:\n",
        "\n",
        "1. What is the number of sneaker images in Y_train_mini?\n",
        "2. What is the number of non-sneaker images in Y_train_mini?\n",
        "3. What is the majority class in Y_train_mini?\n",
        "4. What is the accuracy of a majority class classifier for Y_train_mini?\n",
        "5. Implement a function that computes the Log Loss (binary cross-entropy) metric and use it to evaluate this baseline on both the mini train (Y_train_mini) and validation (Y_val) data. Use 0.1 as the predicted probability for your baseline (reflecting what we know about the original distribution of classes in the mini training data). Hint: for additional help, see the file ``04 Logistic Regression with Tensorflow_helpers.ipynb``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9UXPqPhtRso"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "from sklearn.metrics import log_loss\n",
        "# 1:\n",
        "num_sneaker_images = np.sum(Y_train_mini == 1)\n",
        "print(\"Number of sneaker images in Y_train_mini:\", num_sneaker_images)\n",
        "\n",
        "# 2:\n",
        "num_non_sneaker_images = np.sum(Y_train_mini == 0)\n",
        "print(\"Number of non-sneaker images in Y_train_mini:\", num_non_sneaker_images)\n",
        "\n",
        "# 3:\n",
        "majority_class = np.bincount(Y_train_mini).argmax()\n",
        "print(\"Majority class in Y_train_mini:\", \"Sneaker\" if majority_class == 1 else \"Non-Sneaker\")\n",
        "print(\"Majority class in Y_train_mini:\", \"Sneaker\" if num_sneaker_images >  num_non_sneaker_images else \"Non-Sneaker\")\n",
        "\n",
        "# 4:\n",
        "majority_class_predictions = np.full(Y_train_mini.shape, majority_class)\n",
        "accuracy_mc = np.mean(majority_class_predictions == Y_train_mini)\n",
        "print(f\"Accuracy of the majority class classifier: {accuracy_mc:.4f}\")\n",
        "\n",
        "# 5:\n",
        "def compute_log_loss(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "    log_loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return log_loss\n",
        "\n",
        "# Use 0.1 as the predicted probability for the baseline model\n",
        "baseline_predictions_train = np.full(Y_train_mini.shape, 0.1)\n",
        "baseline_predictions_val = np.full(Y_val.shape, 0.1)\n",
        "\n",
        "# 5: Calculate Log Loss on the mini training and validation datasets\n",
        "log_loss_train = compute_log_loss(Y_train_mini, baseline_predictions_train)\n",
        "log_loss_val = compute_log_loss(Y_val, baseline_predictions_val)\n",
        "\n",
        "print(f\"Log Loss on mini training data: {log_loss_train:.4f}\")\n",
        "print(f\"Log Loss on validation data: {log_loss_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWa3IAlztRso"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 9:</span> Improvement over Baseline with TensorFlow (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh9IHyxRtRso"
      },
      "source": [
        "Let's use TensorFlow to train a binary logistic regression model much like you did in the previous assignment. The goal here is to build a ML model to improve over the baseline classifier.\n",
        "\n",
        "1. Fill in the <span style=\"color:green\">NotImplemented</span> parts of the build_model() function below by following the instructions provided as comments. Hint: the activation function, the loss, and the evaluation metric are different compared to the linear regression model;\n",
        "2. Build and compile a model using the build_model() function and the (X_train_mini, Y_train_mini) data. Set learning_rate = 0.0001. Call the resulting object *model_tf*.\n",
        "3. Train *model_tf* using the (X_train_mini, Y_train_mini) data. Set num_epochs = 5 and batch_size=32. Pass the (X_val, Y_val) data for validation. Hint: see the documentation behind the [tf.keras.Model.fit()](https://bcourses.berkeley.edu/courses/1534588/files/88733489?module_item_id=17073646) method.\n",
        "3. Generate a plot (for the mini training and validation data) with the loss values on the y-axis and the epoch number on the x-axis for visualization. Make sure to include axes name and title. Hint: check what the [tf.keras.Model.fit()](https://bcourses.berkeley.edu/courses/1534588/files/88733489?module_item_id=17073646) method returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHWXsgfhtRso"
      },
      "outputs": [],
      "source": [
        "def build_model(num_features, learning_rate):\n",
        "  \"\"\"Build a TF linear regression model using Keras.\n",
        "\n",
        "  Args:\n",
        "    num_features: The number of input features.\n",
        "    learning_rate: The desired learning rate for SGD.\n",
        "\n",
        "  Returns:\n",
        "    model: A tf.keras model (graph).\n",
        "  \"\"\"\n",
        "  # This is not strictly necessary, but each time you build a model, TF adds\n",
        "  # new nodes (rather than overwriting), so the colab session can end up\n",
        "  # storing lots of copies of the graph when you only care about the most\n",
        "  # recent. Also, as there is some randomness built into training with SGD,\n",
        "  # setting a random seed ensures that results are the same on each identical\n",
        "  # training run.\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  # Build a model using keras.Sequential. While this is intended for neural\n",
        "  # networks (which may have multiple layers), we want just a single layer for\n",
        "  # binary logistic regression.\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=1,        # output dim\n",
        "      input_shape=(num_features,),  # input dim\n",
        "      use_bias=True,               # use a bias (intercept) param\n",
        "      activation='sigmoid',\n",
        "      kernel_initializer=tf.keras.initializers.Ones(),  # initialize params to 1\n",
        "      bias_initializer=tf.keras.initializers.Ones(),    # initialize bias to 1\n",
        "  ))\n",
        "\n",
        "  # We need to choose an optimizer. We'll use SGD, which is actually mini-batch GD\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "  # Finally, compile the model. Select the accuracy metric. This finalizes the graph for training.\n",
        "  model.compile(optimizer=optimizer,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIY504q8tRsp"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(0)\n",
        "# 2. Build and compile model\n",
        "# YOUR CODE HERE\n",
        "num_features = X_train_mini.shape[1]\n",
        "learning_rate = 0.0001\n",
        "model_tf = build_model(num_features, learning_rate)\n",
        "\n",
        "# 3. Fit the model\n",
        "# YOUR CODE HERE\n",
        "history = model_tf.fit(\n",
        "    X_train_mini, Y_train_mini,  # Training data and labels\n",
        "    epochs=5,                    # Number of epochs\n",
        "    batch_size=32,               # Batch size\n",
        "    validation_data=(X_val, Y_val)  # Validation data and labels\n",
        ")\n",
        "\n",
        "# 4. Graph\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Plot the loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, 6), train_loss, label='Training Loss', color='blue')\n",
        "plt.plot(range(1, 6), val_loss, label='Validation Loss', color='red')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO-7hgyJtRsp"
      },
      "source": [
        "---\n",
        "### Step 5: Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a06EttFLtRsp"
      },
      "source": [
        "Hyperparameter tuning is a crucial step in optimizing ML models. It involves systematically adjusting hyperparameters such as learning rate, number of epochs, and optimizer to find the model configuration that leads to the best generalization performance.\n",
        "\n",
        "This tuning process is typically conducted by monitoring the model's performance on the validation vs. training set. It's important to note that using the test set for hyperparameter tuning can compromise the integrity of the evaluation process by violating the assumption of \"blindness\" of the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnfZ6lObtRsp"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 10:</span> Hyperparameter tuning (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5wWT-4KtRsq"
      },
      "source": [
        "1. Fine-tune the **learning rate** and **number of epochs** hyperparameters of *model_tf* to determine the setup that yields the most optimal generalization performance. Feel free to explore various values for these hyperparameters. Hint: you can manually test different hyperparameter values or you can use the [Keras Tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner). If you decide to work with the Keras Tuner, define a new model building function named <span style=\"color:chocolate\">build_model_tuner()</span>.\n",
        "\n",
        "After identifying your preferred model configuration, print the following information:\n",
        "\n",
        "2. The first five learned parameters of the model (this should include the bias term);\n",
        "3. The loss at the final epoch on both the mini training and validation datasets;\n",
        "4. The percentage difference between the losses observed on the mini training and validation datasets.\n",
        "5. Compare the training/validation loss of the TensorFlow model (model_tf) with the baseline model's loss. Does the TensorFlow model demonstrate an improvement over the baseline model?\n",
        "\n",
        "\n",
        "Please note that we will consider 'optimal model configuration' any last-epoch training and validation loss that is below 0.08."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "metadata": {
        "id": "ZJjGsChSHXXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lCAND05VtRsq"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "from keras_tuner import HyperModel, Hyperband\n",
        "# Set the random seed for reproducibility\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# Define the model for hyperparameter tuning\n",
        "def build_model_tuner(hp):\n",
        "    \"\"\"Build a TF logistic regression model using Keras for hyperparameter tuning.\n",
        "\n",
        "    Args:\n",
        "        hp: Keras Tuner Hyperparameter object for tuning learning_rate and epochs.\n",
        "\n",
        "    Returns:\n",
        "        model: A tf.keras model (graph).\n",
        "    \"\"\"\n",
        "    # Clear any previous sessions to avoid memory issues\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(0)\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Add a dense layer for binary classification\n",
        "    model.add(tf.keras.layers.Dense(\n",
        "        units=1,  # Single output unit for binary classification\n",
        "        input_shape=(X_train_mini.shape[1],),  # Input shape corresponds to the number of features (784)\n",
        "        use_bias=True,\n",
        "        activation='sigmoid',  # Sigmoid activation for binary classification\n",
        "        kernel_initializer=tf.keras.initializers.Ones(),  # Initialize weights to 1\n",
        "        bias_initializer=tf.keras.initializers.Ones()  # Initialize bias to 1\n",
        "    ))\n",
        "\n",
        "    # Hyperparameters for tuning\n",
        "    learning_rate = hp.Choice('learning_rate', values=[1e-1, 1e-2, 5e-2, 1e-3, 5e-3, 1e-4, 1e-5])\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Set up the hyperparameter tuning using Keras Tuner (Hyperband)\n",
        "tuner = Hyperband(\n",
        "    build_model_tuner,\n",
        "    objective='val_loss',  # Optimize for validation loss\n",
        "    max_epochs=10,         # Maximum number of epochs to search through\n",
        "    factor=3,              # The factor to increase the number of epochs\n",
        "    directory='hyperparameter_tuning',  # Directory to store the results\n",
        "    project_name='logistic_regression_tuning'\n",
        ")\n",
        "\n",
        "# Set up early stopping to avoid overfitting\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Perform hyperparameter search\n",
        "tuner.search(X_train_mini, Y_train_mini, epochs=50, validation_data=(X_val, Y_val), callbacks=[stop_early], verbose=0)\n",
        "\n",
        "# Retrieve the best model and hyperparameters\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Print the best hyperparameters found by the tuner\n",
        "print(\"Best hyperparameters:\", best_hyperparameters.values)\n",
        "\n",
        "# Determine the best number of epochs by training the model with the best hyperparameters\n",
        "best_model = tuner.hypermodel.build(best_hyperparameters)\n",
        "history = best_model.fit(X_train_mini, Y_train_mini, epochs=50, validation_data=(X_val, Y_val), verbose=1)\n",
        "\n",
        "# Find the best epoch based on validation loss\n",
        "val_loss_per_epoch = history.history['val_loss']\n",
        "best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
        "print(f'Best epoch: {best_epoch}')\n",
        "\n",
        "# Retraining the model with the optimal epoch\n",
        "best_model = tuner.hypermodel.build(best_hyperparameters)\n",
        "history = best_model.fit(X_train_mini, Y_train_mini, epochs=best_epoch, validation_data=(X_val, Y_val), verbose=1)\n",
        "\n",
        "# Retrieve the learned parameters (weights and bias) of the best model\n",
        "weights, bias = best_model.layers[0].get_weights()\n",
        "print(\"Learned Parameters (Weights):\", weights)\n",
        "print(\"Learned Bias Term:\", bias)\n",
        "\n",
        "# Printing the final training and validation loss at the best epoch\n",
        "final_train_loss = best_model.history.history['loss'][-1]\n",
        "final_val_loss = best_model.history.history['val_loss'][-1]\n",
        "print(f\"Final Training Loss: {final_train_loss}\")\n",
        "print(f\"Final Validation Loss: {final_val_loss}\")\n",
        "\n",
        "# Printing the difference between the last-epoch training and validation losses\n",
        "loss_difference = final_train_loss - final_val_loss\n",
        "print(f\"Loss Difference (Train - Validation): {loss_difference}\")\n",
        "\n",
        "# Plotting the training and validation loss curves\n",
        "plt.plot(best_model.history.history['loss'], label='Training Loss')\n",
        "plt.plot(best_model.history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.\n",
        "weights, bias = best_model.layers[0].get_weights()\n",
        "print(\"First five learned parameters (weights and bias):\")\n",
        "print(f\"Weights: \\n{weights[:5]}\")\n",
        "print(f\"Bias: \\n{bias[0]}\")\n",
        "\n",
        "# 2.\n",
        "final_train_loss = best_model.history.history['loss'][-1]\n",
        "final_val_loss = best_model.history.history['val_loss'][-1]\n",
        "print(f\"Final Training Loss: {final_train_loss}\")\n",
        "print(f\"Final Validation Loss: {final_val_loss}\")\n",
        "\n",
        "# 3.\n",
        "loss_difference_percentage = ((final_train_loss - final_val_loss) / final_train_loss) * 100\n",
        "print(f\"Percentage Difference between Training and Validation Loss: {loss_difference_percentage:.2f}%\")\n",
        "\n",
        "# 4.\n",
        "print(f\"Baseline Model Log Loss (Train): {log_loss_train:.4f}\")\n",
        "print(f\"Baseline Model Log Loss (Validation): {log_loss_val:.4f}\")\n",
        "\n",
        "train_loss_diff_percentage = ((final_train_loss - log_loss_train) / log_loss_train) * 100\n",
        "val_loss_diff_percentage = ((final_val_loss - log_loss_val) / log_loss_val) * 100\n",
        "\n",
        "print(f\"Percentage Difference between Training Loss (final vs baseline): {train_loss_diff_percentage:.2f}%\")\n",
        "print(f\"Percentage Difference between Validation Loss (final vs baseline): {val_loss_diff_percentage:.2f}%\")\n",
        "\n",
        "if final_val_loss < 0.08:\n",
        "    print(\"The TensorFlow model demonstrates an improvement over the baseline model.\")\n",
        "else:\n",
        "    print(\"The TensorFlow model does NOT demonstrate an improvement over the baseline model.\")\n"
      ],
      "metadata": {
        "id": "JiU6AjQuJTW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAcNXNqvtRsq"
      },
      "source": [
        "---\n",
        "### Step 6: Evaluation and Generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xADMkEt4tRsq"
      },
      "source": [
        "\n",
        "Now that you've determined the optimal set of hyperparameters, it's time to evaluate your optimized model on the test data to gauge its performance in real-world scenarios, commonly known as inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fMTHFQbtRsq"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 11:</span> Computing accuracy (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENccqzvqtRsr"
      },
      "source": [
        "1. Calculate aggregate accuracy on both mini train and test datasets using a probability threshold of 0.5. Hint: You can utilize the <span style=\"color:chocolate\">model.evaluate()</span> method provided by tf.keras. Note: Aggregate accuracy measures the overall correctness of the model across all classes in the dataset;\n",
        "\n",
        "2. Does the model demonstrate strong aggregate generalization capabilities? Provide an explanation based on your accuracy observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGzEliEMtRsr"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Evaluate the model on the mini training dataset\n",
        "train_loss, train_accuracy = best_model.evaluate(X_train_mini, Y_train_mini, verbose=0)\n",
        "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# 3. Aggregate accuracy on both mini training and test datasets using a threshold of 0.5\n",
        "train_preds = best_model.predict(X_train_mini)\n",
        "test_preds = best_model.predict(X_test)\n",
        "train_preds_binary = (train_preds >= 0.5).astype(int)\n",
        "test_preds_binary = (test_preds >= 0.5).astype(int)\n",
        "train_accuracy_manual = np.mean(train_preds_binary == Y_train_mini)\n",
        "test_accuracy_manual = np.mean(test_preds_binary == Y_test)\n",
        "\n",
        "print(f\"Aggregate Training Accuracy with threshold 0.5: {train_accuracy_manual * 100:.2f}%\")\n",
        "print(f\"Aggregate Test Accuracy with threshold 0.5: {test_accuracy_manual * 100:.2f}%\")\n",
        "\n",
        "print(\"The aggregate training accuracy of the model with a threshold of 0.5 is 82.12%. \\n\"\n",
        "      \"The aggregate test accuracy is 81.85%. \\n\"\n",
        "      \"These results suggest that the model has good generalization capabilities, as the test accuracy is close to the training accuracy. \\n\"\n",
        "      \"With accuracy above 80%, the model demonstrates solid performance in distinguishing between sneaker and non-sneaker images. \\n\"\n",
        "      \"This indicates that the model is not overfitting and is effectively generalizing to unseen data.\"\n",
        "      \"The fact that the accuracies are not identical also suggests the model is not overfitting.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg6L2tP5tRsr"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 12:</span> Fairness evaluation (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7DVnJVutRsr"
      },
      "source": [
        "1. Generate and visualize the confusion matrix on the test dataset using a probability threshold of 0.5. Additionally, print the True Positives (TP), False Negatives (FN), False Positives (FP), and True Negatives (TN). Hint: you can utilize the <span style=\"color:chocolate\">model.predict()</span> method available in tf.keras, and then the <span style=\"color:chocolate\">confusion_matrix()</span>, <span style=\"color:chocolate\">ConfusionMatrixDisplay()</span> methods available in sklearn.metrics;\n",
        "\n",
        "2. Compute subgroup accuracy, separately for the sneaker and non-sneaker classes, on the test dataset using a probability threshold of 0.5. Reflect on any observed accuracy differences (potential lack of fairness) between the two classes.\n",
        "\n",
        "3. Does the model demonstrate strong subgroup generalization capabilities? Provide an explanation based on your accuracy observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h5kaY6jtRss"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# 1.\n",
        "cm = confusion_matrix(Y_test, test_preds_binary)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Non-Sneaker', 'Sneaker'])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix on Test Dataset\")\n",
        "plt.show()\n",
        "\n",
        "# Print TP, FN, FP, TN\n",
        "TP = cm[1, 1]\n",
        "FN = cm[1, 0]\n",
        "FP = cm[0, 1]\n",
        "TN = cm[0, 0]\n",
        "\n",
        "print(f\"True Positives (TP): {TP}\")\n",
        "print(f\"False Negatives (FN): {FN}\")\n",
        "print(f\"False Positives (FP): {FP}\")\n",
        "print(f\"True Negatives (TN): {TN}\")\n",
        "\n",
        "# 2.\n",
        "sneaker_accuracy = np.mean(test_preds_binary[Y_test == 1] == 1)\n",
        "non_sneaker_accuracy = np.mean(test_preds_binary[Y_test == 0] == 0)\n",
        "\n",
        "print(f\"Sneaker class accuracy: {sneaker_accuracy * 100:.2f}%\")\n",
        "print(f\"Non-sneaker class accuracy: {non_sneaker_accuracy * 100:.2f}%\")\n",
        "\n",
        "print(\"There is a noticeable difference in accuracy between the sneaker (91.80%) and non-sneaker (98.88%) classes.\\n\"\n",
        "      \"The difference suggests that the model performs much better on the non-sneaker class.\\n\"\n",
        "      \"This ~7% difference may indicate that the model has a bias toward predicting non-sneaker images correctly, while it tends to make more mistakes on sneaker images.\\n\"\n",
        "      \"That is, there is a potential lack of fairness. This is most likely due to the class imbalance or the model's tendency to favor the majority class.\\n\")\n",
        "\n",
        "# 3.\n",
        "print(\"While the model achieves high overall accuracy on both classes, the stronger performance on the non-sneaker class and the slight underperformance on the sneaker class\\n\"\n",
        "      \"suggest that the model's subgroup generalization capabilities may need improvement.\\n\"\n",
        "      \"The model could be further tuned or balanced (via techniques like class weighting or oversampling on the sneaker class) to ensure more equitable performance across both classes.\\n\"\n",
        "      \"Nonetheless, for each subclass, the model demonstrates strong generalization capabilities, as indicated by the confusion matrix and accuracy values.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq_jY9cgtRss"
      },
      "source": [
        "----\n",
        "#### <span style=\"color:chocolate\">Additional practice question</span> (not graded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-lkddc_tRss"
      },
      "source": [
        "Is it possible to enhance the prediction accuracy for the sneaker class by performing the following steps?\n",
        "\n",
        "1. Implement data balancing techniques, such as oversampling or undersampling, to equalize the representation of both classes.\n",
        "2. After balancing the data, retrain the model on the balanced dataset.\n",
        "3. Evaluate the model's performance, particularly focusing on the accuracy achieved for the sneaker class."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Yes, it is possible to enhance prediction accuracy for the sneaker class by implementing data balancing techniques like oversampling or undersampling. Oversampling increases the number of sneaker class samples, while undersampling reduces the number of non-sneaker class samples to ensure both classes are equally represented. This can help the model focus more on learning the sneaker class, improving accuracy for that class."
      ],
      "metadata": {
        "id": "NStOI60FQaq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "smote = SMOTE(random_state=1234)\n",
        "X_train_balanced, Y_train_balanced = smote.fit_resample(X_train_mini, Y_train_mini)\n",
        "model_tf = build_model(X_train_balanced.shape[1], learning_rate=0.0001)\n",
        "history_balanced = model_tf.fit(X_train_balanced, Y_train_balanced, epochs=5, batch_size=32, validation_data=(X_val, Y_val))\n",
        "Y_test_pred = (model_tf.predict(X_test) > 0.5).astype(\"int32\")\n",
        "test_accuracy = accuracy_score(Y_test, Y_test_pred)\n",
        "print(f\"Test Accuracy with balanced data: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "cm = confusion_matrix(Y_test, Y_test_pred)\n",
        "cm_display = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "cm_display.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix for Test Dataset\")\n",
        "plt.show()\n",
        "\n",
        "sneaker_accuracy = (Y_test_pred[Y_test == 1] == 1).mean()\n",
        "non_sneaker_accuracy = (Y_test_pred[Y_test == 0] == 0).mean()\n",
        "\n",
        "print(f\"Sneaker class accuracy with balanced data: {sneaker_accuracy * 100:.2f}%\")\n",
        "print(f\"Non-sneaker class accuracy with balanced data: {non_sneaker_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "pmqa1FxuQwnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. I did something wrong above."
      ],
      "metadata": {
        "id": "n0NVjR9iRmoW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "copyright",
        "LOcjWKAljbqr",
        "bCBT54r7k2YL",
        "loqjuOZFlEdt"
      ],
      "name": "04 Logistic Regression with Tensorflow.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}